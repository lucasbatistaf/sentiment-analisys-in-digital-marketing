{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50124f4b-9cb8-4eb0-a92a-224779c74203",
   "metadata": {},
   "source": [
    "# Pré-processamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "776cb954-a7b0-452b-bc09-21022cae4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword = set(stopwords.words('portuguese'))\n",
    "stopword.discard(\"não\")\n",
    "stopword.discard(\"nem\")\n",
    "stopword.discard(\"mas\")\n",
    "stemmer = nltk.stem.SnowballStemmer(\"portuguese\")\n",
    "lemma = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'as', 'houvéssemos', 'sou', 'ele', 'tenham', 'haja', 'também', 'me', 'teria', 'houveram', 'houveria', 'que', 'houvemos', 'depois', 'tenho', 'mesmo', 'nós', 'o', 'houveríamos', 'elas', 'isto', 'estou', 'por', 'teve', 'de', 'teriam', 'tiver', 'seu', 'sem', 'teus', 'eram', 'do', 'fôramos', 'são', 'tiveram', 'houvermos', 'hajam', 'houvera', 'tiverem', 'quando', 'forem', 'uma', 'houvéramos', 'muito', 'estava', 'fomos', 'estivéssemos', 'numa', 'sejam', 'éramos', 'nossas', 'seriam', 'aquele', 'hão', 'houverá', 'estes', 'sejamos', 'terei', 'fôssemos', 'tivermos', 'houveriam', 'dos', 'os', 'esteja', 'na', 'às', 'houve', 'hei', 'tém', 'estivera', 'como', 'formos', 'minha', 'aquilo', 'haver', 'no', 'um', 'nosso', 'nossa', 'houveremos', 'aquela', 'tivessem', 'isso', 'pela', 'dela', 'tua', 'tu', 'ao', 'estivermos', 'pelos', 'lhe', 'aquelas', 'estas', 'estavam', 'houver', 'estão', 'até', 'esta', 'houverão', 'estiverem', 'tínhamos', 'estive', 'será', 'seríamos', 'com', 'a', 'havemos', 'para', 'houvessem', 'suas', 'estivéramos', 'tivéramos', 'estivesse', 'teu', 'hajamos', 'temos', 'tivera', 'vos', 'minhas', 'terão', 'te', 'serei', 'seja', 'nas', 'à', 'tivesse', 'tenha', 'tivemos', 'estar', 'seria', 'está', 'fosse', 'ela', 'lhes', 'estivessem', 'já', 'tive', 'estiver', 'era', 'foram', 'tem', 'esse', 'em', 'houverei', 'num', 'entre', 'este', 'esses', 'é', 'tivéssemos', 'estivemos', 'esteve', 'foi', 'se', 'delas', 'dele', 'eu', 'estávamos', 'tinha', 'fora', 'tenhamos', 'pelo', 'houverem', 'somos', 'meus', 'aos', 'seremos', 'teremos', 'das', 'mais', 'ser', 'terá', 'eles', 'tuas', 'nos', 'ou', 'quem', 'teríamos', 'houvesse', 'deles', 'qual', 'vocês', 'meu', 'só', 'tinham', 'fui', 'da', 'seus', 'você', 'fossem', 'estamos', 'estejamos', 'há', 'essa', 'for', 'aqueles', 'estiveram', 'nossos', 'sua', 'serão', 'e', 'estejam', 'essas', 'pelas'}\n"
     ]
    }
   ],
   "source": [
    "print(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo funções\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabet(text):\n",
    "    return re.sub(r'[^a-zA-Z_À-ÿ]', ' ', text)\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    t = [token for token in text if token not in stopword]\n",
    "    text = ' '.join(t)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento do arquivo .csv inicial e exclusão e troca de nome de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimento</th>\n",
       "      <th>comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimento                                        comentarios\n",
       "0           4                                                NaN\n",
       "1           5                                                NaN\n",
       "2           5                                                NaN\n",
       "3           5              Recebi bem antes do prazo estipulado.\n",
       "4           5  Parabéns lojas lannister adorei comprar pela I..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset = dataset.drop(dataset.columns[[0,1,3,5,6]], axis=1)\n",
    "dataset = dataset.rename(columns={'review_score': 'sentimento', 'review_comment_message': 'comentarios'})\n",
    "dataset['sentimento'] = dataset['sentimento'].astype('int')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7290e-b21d-405a-ba3a-f3504258b1c4",
   "metadata": {},
   "source": [
    "\n",
    "Removendo linhas sem comentários, linhas com apenas números ou pontuações e linhas com comentários duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99650419-dab6-4266-b0ba-c9fc677d410e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimento</th>\n",
       "      <th>comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>aparelho eficiente. no site a marca do aparelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Mas um pouco ,travando...pelo valor ta Boa.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>Vendedor confiável, produto ok e entrega antes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentimento                                        comentarios\n",
       "3            5              Recebi bem antes do prazo estipulado.\n",
       "4            5  Parabéns lojas lannister adorei comprar pela I...\n",
       "9            4  aparelho eficiente. no site a marca do aparelh...\n",
       "12           4    Mas um pouco ,travando...pelo valor ta Boa.\\r\\n\n",
       "15           5  Vendedor confiável, produto ok e entrega antes..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if not row['comentarios'].upper().isupper():\n",
    "        dataset = dataset.drop(index)\n",
    "\n",
    "dataset = dataset.drop_duplicates(subset=None , keep='first')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troca dos valores númericos na coluna \"sentimento\" para string de acordo com duas regras propostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset.copy()\n",
    "dataset_2 = dataset.copy()\n",
    "\n",
    "#dataset_1 representa os comentários onde 5 e 4 são positivos, 3 neutro e 2 e 1 negativos\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].replace([5, 4], 'positive')\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].apply(remove_non_alphabet)\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].replace([2, 1], 'negative')\n",
    "\n",
    "#dataset_2 representa os comentários onde 5 é positivo, 4, 3 e 2 são neutros e 1 é negativo\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace(5, 'positive')\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace([4, 3, 2], 'neutral')\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace(1, 'negative')\n",
    "\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stopwords, espaços em branco, não alfabéticos e lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimento</th>\n",
       "      <th>comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>recebi bem antes prazo estipulado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>parabéns lojas lannister adorei comprar intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>aparelho eficiente site marca aparelho impress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>mas pouco travando valor ta boa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>vendedor confiável produto ok entrega antes prazo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentimento                                        comentarios\n",
       "0   positive                  recebi bem antes prazo estipulado\n",
       "1   positive  parabéns lojas lannister adorei comprar intern...\n",
       "2   positive  aparelho eficiente site marca aparelho impress...\n",
       "3   positive                    mas pouco travando valor ta boa\n",
       "4   positive  vendedor confiável produto ok entrega antes prazo"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1_no_stopword = dataset_1.copy()\n",
    "dataset_1_with_stopword = dataset_1.copy()\n",
    "\n",
    "dataset_2_no_stopword = dataset_2.copy()\n",
    "dataset_2_with_stopword = dataset_2.copy()\n",
    "\n",
    "# removing stopwords, extra space\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(word_tokenize)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_stopwords)\n",
    "\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(word_tokenize)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "#with stopwords\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "\n",
    "\n",
    "# remove células que não não NaN porém estão vazias, como por exemplo, uma célula que tem apenas \"espaço\", essa remoção deve ser feita após a limpeza\n",
    "dataset_1_no_stopword = dataset_1_no_stopword[dataset_1_no_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_2_no_stopword = dataset_2_no_stopword[dataset_2_no_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_1_with_stopword = dataset_1_with_stopword[dataset_1_with_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_2_with_stopword = dataset_2_with_stopword[dataset_2_with_stopword['comentarios'].str.strip().astype(bool)]\n",
    "\n",
    "\n",
    "# reseta o indice por questões de compatibilidade\n",
    "dataset_1_no_stopword = dataset_1_no_stopword.reset_index().drop(columns='index')\n",
    "dataset_2_no_stopword = dataset_2_no_stopword.reset_index().drop(columns='index')\n",
    "dataset_1_with_stopword = dataset_1_with_stopword.reset_index().drop(columns='index')\n",
    "dataset_2_with_stopword = dataset_2_with_stopword.reset_index().drop(columns='index')\n",
    "\n",
    "\n",
    "dataset_1_no_stopword.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_no_stopword.to_csv('./Clean Datasets/dataset_1_no_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword.to_csv('./Clean Datasets/dataset_1_with_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_2_no_stopword.to_csv('./Clean Datasets/dataset_2_no_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword.to_csv('./Clean Datasets/dataset_2_with_stopword.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copiando os datasets para lemmatização e stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma\n",
    "\n",
    "dataset_1_no_stopword_lemma = dataset_1_no_stopword.copy()\n",
    "dataset_1_with_stopword_lemma = dataset_1_with_stopword.copy()\n",
    "\n",
    "dataset_2_no_stopword_lemma = dataset_2_no_stopword.copy()\n",
    "dataset_2_with_stopword_lemma = dataset_2_with_stopword.copy()\n",
    "\n",
    "#Stemmer\n",
    "\n",
    "dataset_1_no_stopword_stemm = dataset_1_no_stopword.copy()\n",
    "dataset_1_with_stopword_stemm = dataset_1_with_stopword.copy()\n",
    "\n",
    "dataset_2_no_stopword_stemm = dataset_2_no_stopword.copy()\n",
    "dataset_2_with_stopword_stemm = dataset_2_with_stopword.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f62bd",
   "metadata": {},
   "source": [
    "# Lemmatização dos tokens utilizando SpacY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57daeda0",
   "metadata": {},
   "source": [
    "a lematização não requer que as palavras estejam em formato de token, porém após a lemmatização os dataset passaram pelo processo de tokenization para a aplicação do Bag of Words posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimento</th>\n",
       "      <th>comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[recebir, bem, antes, prazo, estipular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>[parabéns, loja, lannister, adorar, comprar, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[aparelho, eficiente, site, marca, aparelho, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>[mas, pouco, travar, valor, ta, boa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[vendedor, confiável, produto, ok, entregar, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentimento                                        comentarios\n",
       "0   positive            [recebir, bem, antes, prazo, estipular]\n",
       "1   positive  [parabéns, loja, lannister, adorar, comprar, I...\n",
       "2   positive  [aparelho, eficiente, site, marca, aparelho, i...\n",
       "3   positive               [mas, pouco, travar, valor, ta, boa]\n",
       "4   positive  [vendedor, confiável, produto, ok, entregar, a..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_lemma['comentarios'] = dataset_1_no_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_1_no_stopword_lemma['comentarios'] = dataset_1_no_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_1_with_stopword_lemma['comentarios'] = dataset_1_with_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_1_with_stopword_lemma['comentarios'] = dataset_1_with_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_lemma['comentarios'] = dataset_2_no_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_2_no_stopword_lemma['comentarios'] = dataset_2_no_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_2_with_stopword_lemma['comentarios'] = dataset_2_with_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_2_with_stopword_lemma['comentarios'] = dataset_2_with_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_1_no_stopword_lemma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883d5d9",
   "metadata": {},
   "source": [
    "# Stematização com NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3af06f",
   "metadata": {},
   "source": [
    "Para a stematização é necessário que as palavras estejam em formato de lista e pode ser feito através da tokenização dessas palavras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimento</th>\n",
       "      <th>comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>[receb, bem, antes, praz, estipul]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>[parabéns, loj, lannist, ador, compr, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[aparelh, eficient, sit, marc, aparelh, impres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>[mas, pouc, trav, valor, ta, boa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[vendedor, confiável, produt, ok, entreg, ante...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentimento                                        comentarios\n",
       "0   positive                 [receb, bem, antes, praz, estipul]\n",
       "1   positive  [parabéns, loj, lannist, ador, compr, internet...\n",
       "2   positive  [aparelh, eficient, sit, marc, aparelh, impres...\n",
       "3   positive                  [mas, pouc, trav, valor, ta, boa]\n",
       "4   positive  [vendedor, confiável, produt, ok, entreg, ante..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_stemm['comentarios'] = dataset_1_no_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_1_no_stopword_stemm['comentarios'] = dataset_1_no_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_1_with_stopword_stemm['comentarios'] = dataset_1_with_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_1_with_stopword_stemm ['comentarios'] = dataset_1_with_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_stemm['comentarios'] = dataset_2_no_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_2_no_stopword_stemm['comentarios'] = dataset_2_no_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_2_with_stopword_stemm['comentarios'] = dataset_2_with_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_2_with_stopword_stemm['comentarios'] = dataset_2_with_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_1_no_stopword_stemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1389c99",
   "metadata": {},
   "source": [
    "Salvando os datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ec6634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_1_no_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_2_no_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
