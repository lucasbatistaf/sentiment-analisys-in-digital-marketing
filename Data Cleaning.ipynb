{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50124f4b-9cb8-4eb0-a92a-224779c74203",
   "metadata": {},
   "source": [
    "# Pré-processamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cb954-a7b0-452b-bc09-21022cae4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword = set(stopwords.words('portuguese'))\n",
    "stopword.discard(\"não\")\n",
    "stopword.discard(\"nem\")\n",
    "stopword.discard(\"mas\")\n",
    "stemmer = nltk.stem.SnowballStemmer(\"portuguese\")\n",
    "lemma = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo funções\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabet(text):\n",
    "    return re.sub(r'[^a-zA-Z_À-ÿ]', ' ', text)\n",
    "\n",
    "def remove_extra_whitespaces(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    t = [token for token in text if token not in stopword]\n",
    "    text = ' '.join(t)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento do arquivo .csv inicial e exclusão e troca de nome de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "dataset = dataset.drop(dataset.columns[[0,1,3,5,6]], axis=1)\n",
    "dataset = dataset.rename(columns={'review_score': 'sentimento', 'review_comment_message': 'comentarios'})\n",
    "dataset['sentimento'] = dataset['sentimento'].astype('int')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7290e-b21d-405a-ba3a-f3504258b1c4",
   "metadata": {},
   "source": [
    "\n",
    "Removendo linhas sem comentários, linhas com apenas números ou pontuações e linhas com comentários duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99650419-dab6-4266-b0ba-c9fc677d410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if not row['comentarios'].upper().isupper():\n",
    "        dataset = dataset.drop(index)\n",
    "\n",
    "dataset = dataset.drop_duplicates(subset=None , keep='first')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troca dos valores númericos na coluna \"sentimento\" para string de acordo com duas regras propostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset.copy()\n",
    "dataset_2 = dataset.copy()\n",
    "\n",
    "#dataset_1 representa os comentários onde 5 e 4 são positivos, 3 neutro e 2 e 1 negativos\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].replace([5, 4], 'positive')\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].replace(3, 'neutral')\n",
    "dataset_1['sentimento'] = dataset_1['sentimento'].replace([2, 1], 'negative')\n",
    "\n",
    "#dataset_2 representa os comentários onde 5 é positivo, 4, 3 e 2 são neutros e 1 é negativo\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace(5, 'positive')\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace([4, 3, 2], 'neutral')\n",
    "dataset_2['sentimento'] = dataset_2['sentimento'].replace(1, 'negative')\n",
    "\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stopwords, espaços em branco, não alfabéticos e lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_no_stopword = dataset_1.copy()\n",
    "dataset_1_with_stopword = dataset_1.copy()\n",
    "\n",
    "dataset_2_no_stopword = dataset_2.copy()\n",
    "dataset_2_with_stopword = dataset_2.copy()\n",
    "\n",
    "# removing stopwords, extra space\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(word_tokenize)\n",
    "dataset_1_no_stopword['comentarios'] = dataset_1_no_stopword['comentarios'].apply(remove_stopwords)\n",
    "\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(word_tokenize)\n",
    "dataset_2_no_stopword['comentarios'] = dataset_2_no_stopword['comentarios'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "#with stopwords\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_1_with_stopword['comentarios'] = dataset_1_with_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(lambda x: x.lower())\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(remove_non_alphabet)\n",
    "dataset_2_with_stopword['comentarios'] = dataset_2_with_stopword['comentarios'].apply(remove_extra_whitespaces)\n",
    "\n",
    "\n",
    "# remove células que não não NaN porém estão vazias, como por exemplo, uma célula que tem apenas \"espaço\", essa remoção deve ser feita após a limpeza\n",
    "dataset_1_no_stopword = dataset_1_no_stopword[dataset_1_no_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_2_no_stopword = dataset_2_no_stopword[dataset_2_no_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_1_with_stopword = dataset_1_with_stopword[dataset_1_with_stopword['comentarios'].str.strip().astype(bool)]\n",
    "dataset_2_with_stopword = dataset_2_with_stopword[dataset_2_with_stopword['comentarios'].str.strip().astype(bool)]\n",
    "\n",
    "\n",
    "# reseta o indice por questões de compatibilidade\n",
    "dataset_1_no_stopword = dataset_1_no_stopword.reset_index().drop(columns='index')\n",
    "dataset_2_no_stopword = dataset_2_no_stopword.reset_index().drop(columns='index')\n",
    "\n",
    "dataset_1_with_stopword = dataset_1_with_stopword.reset_index().drop(columns='index')\n",
    "dataset_2_with_stopword = dataset_2_with_stopword.reset_index().drop(columns='index')\n",
    "\n",
    "\n",
    "dataset_1_no_stopword.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_no_stopword.to_csv('./Clean Datasets/dataset_1_no_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword.to_csv('./Clean Datasets/dataset_1_with_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_2_no_stopword.to_csv('./Clean Datasets/dataset_2_no_stopword.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword.to_csv('./Clean Datasets/dataset_2_with_stopword.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copiando os datasets para lemmatização e stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma\n",
    "\n",
    "dataset_1_no_stopword_lemma = dataset_1_no_stopword.copy()\n",
    "dataset_1_with_stopword_lemma = dataset_1_with_stopword.copy()\n",
    "\n",
    "dataset_2_no_stopword_lemma = dataset_2_no_stopword.copy()\n",
    "dataset_2_with_stopword_lemma = dataset_2_with_stopword.copy()\n",
    "\n",
    "#Stemmer\n",
    "\n",
    "dataset_1_no_stopword_stemm = dataset_1_no_stopword.copy()\n",
    "dataset_1_with_stopword_stemm = dataset_1_with_stopword.copy()\n",
    "\n",
    "dataset_2_no_stopword_stemm = dataset_2_no_stopword.copy()\n",
    "dataset_2_with_stopword_stemm = dataset_2_with_stopword.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f62bd",
   "metadata": {},
   "source": [
    "# Lemmatização dos tokens utilizando SpacY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57daeda0",
   "metadata": {},
   "source": [
    "a lematização não requer que as palavras estejam em formato de token, porém após a lemmatização os dataset passaram pelo processo de tokenization para a aplicação do Bag of Words posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_lemma['comentarios'] = dataset_1_no_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_1_no_stopword_lemma['comentarios'] = dataset_1_no_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_1_with_stopword_lemma['comentarios'] = dataset_1_with_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_1_with_stopword_lemma['comentarios'] = dataset_1_with_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_lemma['comentarios'] = dataset_2_no_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_2_no_stopword_lemma['comentarios'] = dataset_2_no_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_2_with_stopword_lemma['comentarios'] = dataset_2_with_stopword_lemma['comentarios'].apply(lambda x: ' '.join([y.lemma_ for y in lemma(x)]))\n",
    "dataset_2_with_stopword_lemma['comentarios'] = dataset_2_with_stopword_lemma['comentarios'].apply(word_tokenize)\n",
    "\n",
    "dataset_1_no_stopword_lemma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883d5d9",
   "metadata": {},
   "source": [
    "# Stematização com NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3af06f",
   "metadata": {},
   "source": [
    "Para a stematização é necessário que as palavras estejam em formato de lista e pode ser feito através da tokenização dessas palavras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_stemm['comentarios'] = dataset_1_no_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_1_no_stopword_stemm['comentarios'] = dataset_1_no_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_1_with_stopword_stemm['comentarios'] = dataset_1_with_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_1_with_stopword_stemm ['comentarios'] = dataset_1_with_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_stemm['comentarios'] = dataset_2_no_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_2_no_stopword_stemm['comentarios'] = dataset_2_no_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_2_with_stopword_stemm['comentarios'] = dataset_2_with_stopword_stemm['comentarios'].apply(word_tokenize)\n",
    "dataset_2_with_stopword_stemm['comentarios'] = dataset_2_with_stopword_stemm['comentarios'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "dataset_1_no_stopword_stemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1389c99",
   "metadata": {},
   "source": [
    "Salvando os datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_1\n",
    "\n",
    "dataset_1_no_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_1_no_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_1_with_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "#dataset_2\n",
    "\n",
    "dataset_2_no_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword_lemma.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_lemma.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "dataset_2_no_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_no_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n",
    "dataset_2_with_stopword_stemm.to_csv('./Post Cleaning Datasets/dataset_1_with_stopword_stemm.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
